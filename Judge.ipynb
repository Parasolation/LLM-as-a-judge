{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3,4,5'\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "from constants_prompt import build_autoj_input\n",
    "\n",
    "model_path_list = ['/root/Qwen1.5-7B-Chat', '/root/Meta-Llama-3-8B-Instruct', '/root/.cache/modelscope/hub/qwen/Qwen1___5-14B-Chat',\n",
    "                   '/root/Nanbeige2-8B-Chat', '/root/.cache/modelscope/hub/ZhipuAI/chatglm3-6b', \n",
    "                   '/root/.cache/modelscope/hub/Shanghai_AI_Laboratory/internlm2-chat-7b',\n",
    "                   '/root/.cache/modelscope/hub/deepseek-ai/deepseek-llm-7b-chat',\n",
    "                   '/root/.cache/modelscope/hub/lockonlvange/autoj-13b-fp16',\n",
    "                   '/root/PandaLM-7B-v1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 11:19:14,903\tWARNING services.py:2009 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 1073741824 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2024-05-29 11:19:16,089\tINFO worker.py:1749 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-29 11:19:17 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='/root/PandaLM-7B-v1', speculative_config=None, tokenizer='/root/PandaLM-7B-v1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/root/PandaLM-7B-v1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-29 11:19:26 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=2103410)\u001b[0m INFO 05-29 11:19:26 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=2103410)\u001b[0m INFO 05-29 11:19:27 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO 05-29 11:19:27 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO 05-29 11:19:29 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=2103410)\u001b[0m INFO 05-29 11:19:29 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "WARNING 05-29 11:19:29 custom_all_reduce.py:65] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[36m(RayWorkerWrapper pid=2103410)\u001b[0m WARNING 05-29 11:19:29 custom_all_reduce.py:65] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 05-29 11:19:47 model_runner.py:175] Loading model weights took 3.1858 GB\n",
      "\u001b[36m(RayWorkerWrapper pid=2103709)\u001b[0m INFO 05-29 11:19:50 model_runner.py:175] Loading model weights took 3.1858 GB\n",
      "\u001b[36m(RayWorkerWrapper pid=2103709)\u001b[0m INFO 05-29 11:19:26 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2103604)\u001b[0m INFO 05-29 11:19:27 selector.py:27] Using FlashAttention-2 backend.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2103709)\u001b[0m INFO 05-29 11:19:29 pynccl_utils.py:43] vLLM is using nccl==2.18.1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2103709)\u001b[0m WARNING 05-29 11:19:29 custom_all_reduce.py:65] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "INFO 05-29 11:19:52 distributed_gpu_executor.py:45] # GPU blocks: 8900, # CPU blocks: 2048\n",
      "INFO 05-29 11:19:53 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-29 11:19:53 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=2103410)\u001b[0m INFO 05-29 11:19:54 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerWrapper pid=2103410)\u001b[0m INFO 05-29 11:19:54 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-29 11:20:00 model_runner.py:1017] Graph capturing finished in 7 secs.\n",
      "\u001b[36m(RayWorkerWrapper pid=2103410)\u001b[0m INFO 05-29 11:20:00 model_runner.py:1017] Graph capturing finished in 6 secs.\n",
      "\u001b[36m(RayWorkerWrapper pid=2103604)\u001b[0m INFO 05-29 11:19:50 model_runner.py:175] Loading model weights took 3.1858 GB\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2103709)\u001b[0m INFO 05-29 11:19:54 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2103709)\u001b[0m INFO 05-29 11:19:54 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = model_path_list[3]\n",
    "llm = LLM(model=model_name_or_path, tensor_parallel_size=4, trust_remote_code=True, tokenizer_mode=\"auto\", dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=16384)\n",
    "model_name = model_name_or_path.rsplit('/', 1)[-1]\n",
    "from utils import extract_pariwise_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1496 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed prompts:   3%|▎         | 44/1392 [00:06<03:22,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:10 scheduler.py:648] Input prompt (3043 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:10 scheduler.py:648] Input prompt (2650 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   4%|▎         | 49/1392 [00:06<02:41,  8.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:11 scheduler.py:648] Input prompt (4579 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:11 scheduler.py:648] Input prompt (4007 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   4%|▍         | 53/1392 [00:07<02:35,  8.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:11 scheduler.py:648] Input prompt (2526 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   4%|▍         | 59/1392 [00:08<02:57,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:12 scheduler.py:648] Input prompt (3018 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:12 scheduler.py:648] Input prompt (2579 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:12 scheduler.py:648] Input prompt (2212 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   4%|▍         | 61/1392 [00:08<02:45,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:12 scheduler.py:648] Input prompt (2990 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   5%|▍         | 66/1392 [00:08<01:54, 11.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:12 scheduler.py:648] Input prompt (3668 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   8%|▊         | 116/1392 [00:17<04:57,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:22 scheduler.py:648] Input prompt (2951 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  10%|▉         | 134/1392 [00:21<03:44,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:25 scheduler.py:648] Input prompt (2195 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  10%|█         | 140/1392 [00:22<04:27,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:27 scheduler.py:648] Input prompt (2082 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  10%|█         | 143/1392 [00:23<03:33,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:27 scheduler.py:648] Input prompt (2417 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  10%|█         | 144/1392 [00:23<03:44,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:27 scheduler.py:648] Input prompt (3075 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:27 scheduler.py:648] Input prompt (3061 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  11%|█         | 149/1392 [00:23<02:30,  8.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:28 scheduler.py:648] Input prompt (3196 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  11%|█         | 154/1392 [00:24<03:14,  6.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:29 scheduler.py:648] Input prompt (4513 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:29 scheduler.py:648] Input prompt (2142 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:29 scheduler.py:648] Input prompt (6896 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:29 scheduler.py:648] Input prompt (2776 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  11%|█▏        | 160/1392 [00:25<02:14,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:29 scheduler.py:648] Input prompt (2399 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:29 scheduler.py:648] Input prompt (2248 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  12%|█▏        | 164/1392 [00:25<02:21,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:30 scheduler.py:648] Input prompt (2062 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:30 scheduler.py:648] Input prompt (2890 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:30 scheduler.py:648] Input prompt (2594 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  12%|█▏        | 173/1392 [00:27<03:05,  6.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:31 scheduler.py:648] Input prompt (2605 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:31 scheduler.py:648] Input prompt (2374 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:31 scheduler.py:648] Input prompt (2356 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:31 scheduler.py:648] Input prompt (2425 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  13%|█▎        | 179/1392 [00:27<01:57, 10.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:32 scheduler.py:648] Input prompt (2410 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  13%|█▎        | 183/1392 [00:28<02:28,  8.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:32 scheduler.py:648] Input prompt (2818 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:32 scheduler.py:648] Input prompt (2717 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:32 scheduler.py:648] Input prompt (2302 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:32 scheduler.py:648] Input prompt (2308 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:32 scheduler.py:648] Input prompt (2550 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:32 scheduler.py:648] Input prompt (2316 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:32 scheduler.py:648] Input prompt (2212 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:32 scheduler.py:648] Input prompt (2267 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  14%|█▍        | 196/1392 [00:29<01:26, 13.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:33 scheduler.py:648] Input prompt (2220 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:33 scheduler.py:648] Input prompt (4273 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  15%|█▍        | 203/1392 [00:29<02:01,  9.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:34 scheduler.py:648] Input prompt (2591 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:34 scheduler.py:648] Input prompt (2596 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  15%|█▌        | 210/1392 [00:31<03:17,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:35 scheduler.py:648] Input prompt (2056 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  15%|█▌        | 211/1392 [00:31<03:49,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:35 scheduler.py:648] Input prompt (2129 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  17%|█▋        | 231/1392 [00:34<03:19,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:38 scheduler.py:648] Input prompt (3099 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:38 scheduler.py:648] Input prompt (3061 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:38 scheduler.py:648] Input prompt (2988 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:38 scheduler.py:648] Input prompt (2375 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:38 scheduler.py:648] Input prompt (4349 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:38 scheduler.py:648] Input prompt (2183 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  18%|█▊        | 244/1392 [00:35<02:28,  7.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:39 scheduler.py:648] Input prompt (2255 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  18%|█▊        | 248/1392 [00:36<03:29,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:40 scheduler.py:648] Input prompt (2741 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  18%|█▊        | 251/1392 [00:37<03:31,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:41 scheduler.py:648] Input prompt (3147 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:41 scheduler.py:648] Input prompt (3835 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  18%|█▊        | 252/1392 [00:37<03:49,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:41 scheduler.py:648] Input prompt (2380 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:41 scheduler.py:648] Input prompt (2355 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:41 scheduler.py:648] Input prompt (2642 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  19%|█▊        | 259/1392 [00:37<02:03,  9.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:20:42 scheduler.py:648] Input prompt (3418 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:42 scheduler.py:648] Input prompt (2443 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:20:42 scheduler.py:648] Input prompt (2336 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  33%|███▎      | 455/1392 [01:00<01:52,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:21:05 scheduler.py:648] Input prompt (2145 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  37%|███▋      | 519/1392 [01:07<01:48,  8.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:21:11 scheduler.py:648] Input prompt (3163 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  38%|███▊      | 523/1392 [01:07<01:31,  9.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:21:12 scheduler.py:648] Input prompt (4709 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  39%|███▉      | 547/1392 [01:10<01:32,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:21:15 scheduler.py:648] Input prompt (2409 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  47%|████▋     | 654/1392 [01:24<02:11,  5.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:21:29 scheduler.py:648] Input prompt (2468 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  60%|█████▉    | 834/1392 [01:49<01:32,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:21:53 scheduler.py:648] Input prompt (2213 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  62%|██████▏   | 862/1392 [01:54<01:37,  5.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:21:58 scheduler.py:648] Input prompt (2062 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  62%|██████▏   | 863/1392 [01:54<01:50,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:21:58 scheduler.py:648] Input prompt (2375 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  66%|██████▋   | 924/1392 [02:04<01:15,  6.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:22:08 scheduler.py:648] Input prompt (2524 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  66%|██████▋   | 925/1392 [02:04<01:27,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:22:09 scheduler.py:648] Input prompt (2603 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  67%|██████▋   | 930/1392 [02:05<00:56,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:22:09 scheduler.py:648] Input prompt (5661 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  70%|██████▉   | 970/1392 [02:10<01:25,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:22:15 scheduler.py:648] Input prompt (2549 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  70%|██████▉   | 971/1392 [02:11<01:26,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:22:15 scheduler.py:648] Input prompt (2109 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  70%|███████   | 978/1392 [02:12<01:10,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:22:16 scheduler.py:648] Input prompt (2624 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:22:16 scheduler.py:648] Input prompt (2733 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  71%|███████   | 982/1392 [02:12<01:02,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:22:17 scheduler.py:648] Input prompt (2785 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  72%|███████▏  | 1005/1392 [02:16<01:19,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:22:21 scheduler.py:648] Input prompt (3243 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  79%|███████▉  | 1105/1392 [02:30<00:30,  9.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:22:34 scheduler.py:648] Input prompt (2287 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  80%|███████▉  | 1109/1392 [02:30<00:30,  9.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:22:34 scheduler.py:648] Input prompt (2521 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:22:34 scheduler.py:648] Input prompt (2108 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:22:34 scheduler.py:648] Input prompt (2070 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  80%|███████▉  | 1110/1392 [02:30<00:35,  8.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:22:35 scheduler.py:648] Input prompt (2456 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:22:35 scheduler.py:648] Input prompt (2774 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  81%|████████  | 1126/1392 [02:32<00:39,  6.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:22:36 scheduler.py:648] Input prompt (2497 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  83%|████████▎ | 1160/1392 [02:36<00:41,  5.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:22:41 scheduler.py:648] Input prompt (2359 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  84%|████████▍ | 1174/1392 [02:39<00:39,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:22:43 scheduler.py:648] Input prompt (2080 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  91%|█████████ | 1266/1392 [02:50<00:16,  7.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:22:54 scheduler.py:648] Input prompt (2157 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  93%|█████████▎| 1290/1392 [02:52<00:10,  9.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:22:57 scheduler.py:648] Input prompt (2172 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  93%|█████████▎| 1299/1392 [02:54<00:12,  7.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:22:58 scheduler.py:648] Input prompt (2171 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  94%|█████████▎| 1304/1392 [02:55<00:15,  5.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:22:59 scheduler.py:648] Input prompt (2781 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  95%|█████████▍| 1316/1392 [02:58<00:18,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:23:02 scheduler.py:648] Input prompt (2241 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  95%|█████████▍| 1317/1392 [02:58<00:19,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:23:02 scheduler.py:648] Input prompt (2281 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  95%|█████████▌| 1327/1392 [03:00<00:13,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:23:04 scheduler.py:648] Input prompt (2971 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  96%|█████████▌| 1333/1392 [03:01<00:14,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:23:05 scheduler.py:648] Input prompt (2679 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:23:05 scheduler.py:648] Input prompt (3421 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  96%|█████████▌| 1337/1392 [03:01<00:07,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:23:06 scheduler.py:648] Input prompt (2791 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  96%|█████████▋| 1340/1392 [03:02<00:08,  6.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:23:06 scheduler.py:648] Input prompt (2308 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:23:06 scheduler.py:648] Input prompt (2470 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:23:06 scheduler.py:648] Input prompt (2998 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  99%|█████████▉| 1376/1392 [03:06<00:02,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:23:10 scheduler.py:648] Input prompt (2545 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  99%|█████████▉| 1385/1392 [03:08<00:01,  4.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:23:12 scheduler.py:648] Input prompt (3052 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1392/1392 [03:09<00:00,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# autoj\n",
    "texts_autoj = []\n",
    "input_file_path = '/root/auto-j/data/test/testdata_pairwise.jsonl'\n",
    "with open(input_file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        prompt = build_autoj_input(prompt=item['prompt'], resp1=item['response 1'], resp2=item['response 2'], protocol=\"pairwise_tie\")\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        texts_autoj.append(text)\n",
    "outputs_autoj = llm.generate(texts_autoj, sampling_params)\n",
    "dataset_name = input_file_path.rsplit('/', 1)[-1].split('.')[0]\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/'\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.mkdir(output_file_path)\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/' + dataset_name + '_response.json'\n",
    "with open(input_file_path, 'r') as fin:\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        for output, line in zip(outputs_autoj, fin):\n",
    "            item = json.loads(line)\n",
    "            pred_label = extract_pariwise_result(output.outputs[0].text)\n",
    "            item['response_judge'] = output.outputs[0].text\n",
    "            item['pred_label'] = pred_label\n",
    "            json.dump(item, f)\n",
    "            f.write('\\n')\n",
    "            print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 999/999 [01:26<00:00, 11.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "# pandalm\n",
    "prompt_templates = {\n",
    "    \"alpaca\": {\n",
    "        \"description\": \"Template used by Alpaca-LoRA.\",\n",
    "        \"prompt_input\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n",
    "        \"prompt_no_input\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n",
    "        \"response_split\": \"### Response:\",\n",
    "    }\n",
    "}\n",
    "def get_prompt(instruction, input):\n",
    "    prompt = prompt_templates['alpaca'][\"prompt_input\"].format(instruction=instruction, input=input)\n",
    "    return prompt\n",
    "\n",
    "\n",
    "texts_pandalm = []\n",
    "    \n",
    "import json\n",
    "input_file_path = '/root/PandaLM/data/testset-v1.json'\n",
    "with open(input_file_path, 'r') as f:\n",
    "    input_data_pandalm = json.load(f)\n",
    "    for item in input_data_pandalm:\n",
    "        prompt = build_autoj_input(prompt=get_prompt(item['instruction'], item['input']), resp1=item['response1'], resp2=item['response2'], protocol=\"pairwise_tie\")\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        texts_pandalm.append(text)\n",
    "\n",
    "outputs_pandalm = llm.generate(texts_pandalm, sampling_params)\n",
    "dataset_name = input_file_path.rsplit('/', 1)[-1].split('.')[0]\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/'\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.mkdir(output_file_path)\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/' + dataset_name + '_response.json'\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for output, item in zip(outputs_pandalm, input_data_pandalm):\n",
    "        pred_label = extract_pariwise_result(output.outputs[0].text)\n",
    "        item['response_judge'] = output.outputs[0].text\n",
    "        item['pred_label'] = pred_label\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')\n",
    "        print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:08<00:00, 12.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# llmbar_natural\n",
    "texts_natural = []\n",
    "input_file_path = '/root/LLMBar/Dataset/LLMBar/Natural/dataset.json'\n",
    "with open(input_file_path, 'r') as f:\n",
    "    input_data_llmbar_natural = json.load(f)\n",
    "    for item in input_data_llmbar_natural:\n",
    "        prompt = build_autoj_input(prompt=item['input'], resp1=item['output_1'], resp2=item['output_2'], protocol=\"pairwise_tie\")\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        texts_natural.append(text)\n",
    "outputs_llmbar_natural = llm.generate(texts_natural, sampling_params)\n",
    "dataset_name = input_file_path.rsplit('/', 1)[-1].split('.')[0]\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/'\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.mkdir(output_file_path)\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/' + dataset_name + '_response.json'\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for output, item in zip(outputs_llmbar_natural, input_data_llmbar_natural):\n",
    "        pred_label = extract_pariwise_result(output.outputs[0].text)\n",
    "        item['response_judge'] = output.outputs[0].text\n",
    "        item['pred_label'] = pred_label\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')\n",
    "        print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 134/134 [00:11<00:00, 11.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# llmbar_neighbor\n",
    "\n",
    "texts_neighbor = []\n",
    "input_file_path = '/root/LLMBar/Dataset/LLMBar/Adversarial/Neighbor/dataset.json'\n",
    "with open(input_file_path, 'r') as f:\n",
    "    input_data_llmbar_neighbor = json.load(f)\n",
    "    for item in input_data_llmbar_neighbor:\n",
    "        prompt = build_autoj_input(prompt=item['input'], resp1=item['output_1'], resp2=item['output_2'], protocol=\"pairwise_tie\")\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        texts_neighbor.append(text)\n",
    "outputs_llmbar_neighbor = llm.generate(texts_neighbor, sampling_params)\n",
    "dataset_name = input_file_path.rsplit('/', 1)[-1].split('.')[0]\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/'\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.mkdir(output_file_path)\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/' + dataset_name + '_response.json'\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for output, item in zip(outputs_llmbar_neighbor, input_data_llmbar_neighbor):\n",
    "        pred_label = extract_pariwise_result(output.outputs[0].text)\n",
    "        item['response_judge'] = output.outputs[0].text\n",
    "        item['pred_label'] = pred_label\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')\n",
    "        print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 92/92 [00:12<00:00,  7.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# llmbar_gptinst\n",
    "\n",
    "texts_gptinst = []\n",
    "input_file_path = '/root/LLMBar/Dataset/LLMBar/Adversarial/GPTInst/dataset.json'\n",
    "with open(input_file_path, 'r') as f:\n",
    "    input_data_llmbar_gptinst = json.load(f)\n",
    "    for item in input_data_llmbar_gptinst:\n",
    "        prompt = build_autoj_input(prompt=item['input'], resp1=item['output_1'], resp2=item['output_2'], protocol=\"pairwise_tie\")\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        texts_gptinst.append(text)\n",
    "outputs_llmbar_gptinst = llm.generate(texts_gptinst, sampling_params)\n",
    "dataset_name = input_file_path.rsplit('/', 1)[-1].split('.')[0]\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/'\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.mkdir(output_file_path)\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/' + dataset_name + '_response.json'\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for output, item in zip(outputs_llmbar_gptinst, input_data_llmbar_gptinst):\n",
    "        pred_label = extract_pariwise_result(output.outputs[0].text)\n",
    "        item['response_judge'] = output.outputs[0].text\n",
    "        item['pred_label'] = pred_label\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')\n",
    "        print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 47/47 [00:03<00:00, 13.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# llmbar_gptout\n",
    "\n",
    "texts_gptout = []\n",
    "input_file_path = '/root/LLMBar/Dataset/LLMBar/Adversarial/GPTOut/dataset.json'\n",
    "with open(input_file_path, 'r') as f:\n",
    "    input_data_llmbar_gptout = json.load(f)\n",
    "    for item in input_data_llmbar_gptout:\n",
    "        prompt = build_autoj_input(prompt=item['input'], resp1=item['output_1'], resp2=item['output_2'], protocol=\"pairwise_tie\")\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        texts_gptout.append(text)\n",
    "outputs_llmbar_gptout = llm.generate(texts_gptout, sampling_params)\n",
    "dataset_name = input_file_path.rsplit('/', 1)[-1].split('.')[0]\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/'\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.mkdir(output_file_path)\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/' + dataset_name + '_response.json'\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for output, item in zip(outputs_llmbar_gptout, input_data_llmbar_gptout):\n",
    "        pred_label = extract_pariwise_result(output.outputs[0].text)\n",
    "        item['response_judge'] = output.outputs[0].text\n",
    "        item['pred_label'] = pred_label\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')\n",
    "        print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 46/46 [00:05<00:00,  8.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# llmbar_manual\n",
    "\n",
    "texts_manual = []\n",
    "input_file_path = '/root/LLMBar/Dataset/LLMBar/Adversarial/Manual/dataset.json'\n",
    "with open(input_file_path, 'r') as f:\n",
    "    input_data_llmbar_manual = json.load(f)\n",
    "    for item in input_data_llmbar_manual:\n",
    "        prompt = build_autoj_input(prompt=item['input'], resp1=item['output_1'], resp2=item['output_2'], protocol=\"pairwise_tie\")\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        texts_manual.append(text)\n",
    "outputs_llmbar_manual = llm.generate(texts_manual, sampling_params)\n",
    "dataset_name = input_file_path.rsplit('/', 1)[-1].split('.')[0]\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/'\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.mkdir(output_file_path)\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/' + dataset_name + '_response.json'\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for output, item in zip(outputs_llmbar_manual, input_data_llmbar_manual):\n",
    "        pred_label = extract_pariwise_result(output.outputs[0].text)\n",
    "        item['response_judge'] = output.outputs[0].text\n",
    "        item['pred_label'] = pred_label\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')\n",
    "        print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 3/3355 [00:00<12:31,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:25:32 scheduler.py:648] Input prompt (2062 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:25:32 scheduler.py:648] Input prompt (2062 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 7/3355 [00:01<07:45,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:25:32 scheduler.py:648] Input prompt (2677 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:25:32 scheduler.py:648] Input prompt (2677 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 15/3355 [00:02<11:38,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:25:34 scheduler.py:648] Input prompt (2397 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:25:34 scheduler.py:648] Input prompt (2397 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:25:34 scheduler.py:648] Input prompt (3088 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:25:34 scheduler.py:648] Input prompt (3088 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:25:34 scheduler.py:648] Input prompt (3088 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:25:34 scheduler.py:648] Input prompt (3088 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:25:34 scheduler.py:648] Input prompt (3103 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:25:34 scheduler.py:648] Input prompt (3103 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:25:34 scheduler.py:648] Input prompt (3143 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:25:34 scheduler.py:648] Input prompt (3143 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:25:34 scheduler.py:648] Input prompt (3143 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:25:34 scheduler.py:648] Input prompt (3143 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   4%|▍         | 136/3355 [00:24<14:42,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:25:55 scheduler.py:648] Input prompt (2057 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:25:55 scheduler.py:648] Input prompt (2057 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:25:55 scheduler.py:648] Input prompt (2057 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:25:55 scheduler.py:648] Input prompt (2057 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:25:55 scheduler.py:648] Input prompt (2591 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:25:55 scheduler.py:648] Input prompt (2591 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  16%|█▌        | 537/3355 [01:28<09:51,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:26:59 scheduler.py:648] Input prompt (2233 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:26:59 scheduler.py:648] Input prompt (2233 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  16%|█▌        | 541/3355 [01:28<07:15,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:27:00 scheduler.py:648] Input prompt (2131 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:27:00 scheduler.py:648] Input prompt (2131 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:27:00 scheduler.py:648] Input prompt (2131 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:27:00 scheduler.py:648] Input prompt (2131 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:27:00 scheduler.py:648] Input prompt (2091 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:27:00 scheduler.py:648] Input prompt (2091 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  16%|█▋        | 553/3355 [01:30<07:26,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:27:01 scheduler.py:648] Input prompt (2233 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:27:01 scheduler.py:648] Input prompt (2233 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  19%|█▉        | 651/3355 [01:47<12:57,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:27:18 scheduler.py:648] Input prompt (2128 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:27:18 scheduler.py:648] Input prompt (2128 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:27:18 scheduler.py:648] Input prompt (2128 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:27:18 scheduler.py:648] Input prompt (2128 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  20%|█▉        | 665/3355 [01:50<12:21,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:27:21 scheduler.py:648] Input prompt (2254 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:27:21 scheduler.py:648] Input prompt (2254 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:27:21 scheduler.py:648] Input prompt (2128 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:27:21 scheduler.py:648] Input prompt (2128 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:27:21 scheduler.py:648] Input prompt (2213 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:27:21 scheduler.py:648] Input prompt (2213 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  20%|██        | 677/3355 [01:51<07:27,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:27:22 scheduler.py:648] Input prompt (2133 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:27:22 scheduler.py:648] Input prompt (2133 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  41%|████      | 1359/3355 [03:21<03:12, 10.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:28:52 scheduler.py:648] Input prompt (2456 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:28:52 scheduler.py:648] Input prompt (2456 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  41%|████      | 1379/3355 [03:24<07:50,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:28:56 scheduler.py:648] Input prompt (2898 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:28:56 scheduler.py:648] Input prompt (2898 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  42%|████▏     | 1394/3355 [03:26<04:29,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:28:58 scheduler.py:648] Input prompt (2983 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:28:58 scheduler.py:648] Input prompt (2983 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:28:58 scheduler.py:648] Input prompt (2445 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:28:58 scheduler.py:648] Input prompt (2445 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  51%|█████     | 1709/3355 [04:17<06:16,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:29:48 scheduler.py:648] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:29:48 scheduler.py:648] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  53%|█████▎    | 1769/3355 [04:31<07:19,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:30:02 scheduler.py:648] Input prompt (2432 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:02 scheduler.py:648] Input prompt (2432 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  53%|█████▎    | 1777/3355 [04:32<06:09,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:30:04 scheduler.py:648] Input prompt (2064 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:04 scheduler.py:648] Input prompt (2064 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:04 scheduler.py:648] Input prompt (2717 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:04 scheduler.py:648] Input prompt (2717 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:04 scheduler.py:648] Input prompt (2717 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:04 scheduler.py:648] Input prompt (2717 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  53%|█████▎    | 1787/3355 [04:33<04:01,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2252 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2252 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2252 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2252 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2252 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2252 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2252 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2252 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2336 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2336 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2336 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2336 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (3100 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (3100 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2363 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2363 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2432 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2432 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2717 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2717 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2989 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2989 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (3100 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (3100 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (3100 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (3100 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (3016 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (3016 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2363 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (2363 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (3016 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:05 scheduler.py:648] Input prompt (3016 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  55%|█████▍    | 1830/3355 [04:36<02:15, 11.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:30:08 scheduler.py:648] Input prompt (2126 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:08 scheduler.py:648] Input prompt (2126 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  55%|█████▌    | 1855/3355 [04:41<04:47,  5.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:30:13 scheduler.py:648] Input prompt (2126 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:13 scheduler.py:648] Input prompt (2126 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:13 scheduler.py:648] Input prompt (2126 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:13 scheduler.py:648] Input prompt (2126 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  56%|█████▌    | 1869/3355 [04:43<04:28,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:30:15 scheduler.py:648] Input prompt (2265 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:15 scheduler.py:648] Input prompt (2265 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:15 scheduler.py:648] Input prompt (2265 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:15 scheduler.py:648] Input prompt (2265 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  56%|█████▌    | 1875/3355 [04:44<02:34,  9.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:30:15 scheduler.py:648] Input prompt (2052 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:15 scheduler.py:648] Input prompt (2052 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:15 scheduler.py:648] Input prompt (2307 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:15 scheduler.py:648] Input prompt (2307 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:15 scheduler.py:648] Input prompt (2307 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:15 scheduler.py:648] Input prompt (2307 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  56%|█████▋    | 1891/3355 [04:46<04:10,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:30:18 scheduler.py:648] Input prompt (2552 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:18 scheduler.py:648] Input prompt (2552 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  56%|█████▋    | 1895/3355 [04:47<03:24,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:30:18 scheduler.py:648] Input prompt (2051 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:18 scheduler.py:648] Input prompt (2051 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  57%|█████▋    | 1899/3355 [04:47<03:08,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:30:19 scheduler.py:648] Input prompt (2112 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:19 scheduler.py:648] Input prompt (2112 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  57%|█████▋    | 1919/3355 [04:52<06:19,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:30:24 scheduler.py:648] Input prompt (2347 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:24 scheduler.py:648] Input prompt (2347 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:24 scheduler.py:648] Input prompt (2112 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:24 scheduler.py:648] Input prompt (2112 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  59%|█████▊    | 1969/3355 [05:02<04:58,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:30:34 scheduler.py:648] Input prompt (2275 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  59%|█████▉    | 1972/3355 [05:03<03:50,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:30:34 scheduler.py:648] Input prompt (2275 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:34 scheduler.py:648] Input prompt (2275 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  59%|█████▉    | 1978/3355 [05:04<04:12,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:30:35 scheduler.py:648] Input prompt (2483 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:35 scheduler.py:648] Input prompt (2483 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:35 scheduler.py:648] Input prompt (2483 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:35 scheduler.py:648] Input prompt (2483 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:35 scheduler.py:648] Input prompt (2483 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:35 scheduler.py:648] Input prompt (2483 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:35 scheduler.py:648] Input prompt (2332 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:35 scheduler.py:648] Input prompt (2332 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  60%|██████    | 2022/3355 [05:12<05:36,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:30:44 scheduler.py:648] Input prompt (2117 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:44 scheduler.py:648] Input prompt (2117 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:44 scheduler.py:648] Input prompt (2117 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:44 scheduler.py:648] Input prompt (2117 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:44 scheduler.py:648] Input prompt (2470 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:30:44 scheduler.py:648] Input prompt (2470 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  72%|███████▏  | 2404/3355 [06:12<03:19,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:31:43 scheduler.py:648] Input prompt (2583 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:43 scheduler.py:648] Input prompt (2583 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:43 scheduler.py:648] Input prompt (2583 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:43 scheduler.py:648] Input prompt (2583 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:43 scheduler.py:648] Input prompt (2583 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:43 scheduler.py:648] Input prompt (2583 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:43 scheduler.py:648] Input prompt (2688 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:43 scheduler.py:648] Input prompt (2688 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  72%|███████▏  | 2418/3355 [06:13<01:13, 12.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2699 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2699 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2668 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2668 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2744 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2744 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2699 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2699 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (4505 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (4505 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2583 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2583 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2713 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2713 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2688 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2688 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2668 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2668 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2091 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2091 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2091 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2091 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2091 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:44 scheduler.py:648] Input prompt (2091 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  73%|███████▎  | 2446/3355 [06:13<00:24, 37.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:31:45 scheduler.py:648] Input prompt (2965 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:45 scheduler.py:648] Input prompt (2965 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  73%|███████▎  | 2451/3355 [06:14<00:42, 21.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:31:45 scheduler.py:648] Input prompt (2502 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:45 scheduler.py:648] Input prompt (2502 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:45 scheduler.py:648] Input prompt (2091 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:45 scheduler.py:648] Input prompt (2091 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  73%|███████▎  | 2458/3355 [06:14<00:50, 17.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:31:46 scheduler.py:648] Input prompt (3159 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:46 scheduler.py:648] Input prompt (3159 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  74%|███████▎  | 2471/3355 [06:17<02:17,  6.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:31:48 scheduler.py:648] Input prompt (2371 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:48 scheduler.py:648] Input prompt (2371 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:48 scheduler.py:648] Input prompt (2502 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:48 scheduler.py:648] Input prompt (2502 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:48 scheduler.py:648] Input prompt (3159 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:48 scheduler.py:648] Input prompt (3159 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:48 scheduler.py:648] Input prompt (3159 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:48 scheduler.py:648] Input prompt (3159 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:48 scheduler.py:648] Input prompt (2511 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:48 scheduler.py:648] Input prompt (2511 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:48 scheduler.py:648] Input prompt (2511 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:48 scheduler.py:648] Input prompt (2511 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:48 scheduler.py:648] Input prompt (2371 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:48 scheduler.py:648] Input prompt (2371 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  74%|███████▍  | 2491/3355 [06:18<01:04, 13.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:31:50 scheduler.py:648] Input prompt (2149 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:50 scheduler.py:648] Input prompt (2149 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  75%|███████▍  | 2512/3355 [06:22<03:15,  4.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:31:54 scheduler.py:648] Input prompt (2075 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:54 scheduler.py:648] Input prompt (2075 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  75%|███████▌  | 2518/3355 [06:23<02:57,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:31:55 scheduler.py:648] Input prompt (2075 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:31:55 scheduler.py:648] Input prompt (2075 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  76%|███████▋  | 2561/3355 [06:34<03:45,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:32:06 scheduler.py:648] Input prompt (2118 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:06 scheduler.py:648] Input prompt (2118 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  76%|███████▋  | 2565/3355 [06:35<02:20,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:32:06 scheduler.py:648] Input prompt (2282 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:06 scheduler.py:648] Input prompt (2282 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  77%|███████▋  | 2585/3355 [06:40<03:36,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:32:11 scheduler.py:648] Input prompt (2542 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  77%|███████▋  | 2588/3355 [06:40<02:45,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:32:12 scheduler.py:648] Input prompt (2576 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:12 scheduler.py:648] Input prompt (2576 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:12 scheduler.py:648] Input prompt (2447 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:12 scheduler.py:648] Input prompt (2447 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:12 scheduler.py:648] Input prompt (2447 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  77%|███████▋  | 2595/3355 [06:41<01:21,  9.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:32:12 scheduler.py:648] Input prompt (2730 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:12 scheduler.py:648] Input prompt (2730 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:12 scheduler.py:648] Input prompt (2647 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:12 scheduler.py:648] Input prompt (2647 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:12 scheduler.py:648] Input prompt (2601 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:12 scheduler.py:648] Input prompt (2601 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:12 scheduler.py:648] Input prompt (2447 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:12 scheduler.py:648] Input prompt (2447 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:12 scheduler.py:648] Input prompt (2625 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:12 scheduler.py:648] Input prompt (2625 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:12 scheduler.py:648] Input prompt (2601 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:12 scheduler.py:648] Input prompt (2601 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  79%|███████▉  | 2651/3355 [06:50<02:54,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:32:21 scheduler.py:648] Input prompt (2553 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:21 scheduler.py:648] Input prompt (2553 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:21 scheduler.py:648] Input prompt (3051 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:21 scheduler.py:648] Input prompt (3051 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:21 scheduler.py:648] Input prompt (3051 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:21 scheduler.py:648] Input prompt (3051 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:21 scheduler.py:648] Input prompt (2783 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:21 scheduler.py:648] Input prompt (2783 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  80%|███████▉  | 2669/3355 [06:52<02:29,  4.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:32:24 scheduler.py:648] Input prompt (2147 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:24 scheduler.py:648] Input prompt (2147 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  80%|████████  | 2686/3355 [06:56<02:56,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:32:28 scheduler.py:648] Input prompt (2139 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:28 scheduler.py:648] Input prompt (2139 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  80%|████████  | 2694/3355 [06:58<02:31,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:32:29 scheduler.py:648] Input prompt (2147 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:29 scheduler.py:648] Input prompt (2147 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:29 scheduler.py:648] Input prompt (2147 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:29 scheduler.py:648] Input prompt (2147 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  80%|████████  | 2700/3355 [06:58<01:19,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:32:30 scheduler.py:648] Input prompt (2139 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:30 scheduler.py:648] Input prompt (2139 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:30 scheduler.py:648] Input prompt (2139 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:30 scheduler.py:648] Input prompt (2139 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  81%|████████  | 2721/3355 [07:03<02:44,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:32:34 scheduler.py:648] Input prompt (2392 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:34 scheduler.py:648] Input prompt (2392 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:34 scheduler.py:648] Input prompt (2392 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:34 scheduler.py:648] Input prompt (2392 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  82%|████████▏ | 2744/3355 [07:07<02:11,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:32:39 scheduler.py:648] Input prompt (2392 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:39 scheduler.py:648] Input prompt (2392 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  82%|████████▏ | 2764/3355 [07:12<02:50,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:32:44 scheduler.py:648] Input prompt (2115 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:44 scheduler.py:648] Input prompt (2115 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:44 scheduler.py:648] Input prompt (2115 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:44 scheduler.py:648] Input prompt (2115 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:44 scheduler.py:648] Input prompt (2115 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:44 scheduler.py:648] Input prompt (2115 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:44 scheduler.py:648] Input prompt (2115 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:44 scheduler.py:648] Input prompt (2115 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  83%|████████▎ | 2780/3355 [07:14<01:47,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:32:46 scheduler.py:648] Input prompt (2115 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:46 scheduler.py:648] Input prompt (2115 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:46 scheduler.py:648] Input prompt (2115 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:46 scheduler.py:648] Input prompt (2115 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  84%|████████▍ | 2824/3355 [07:25<02:16,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:32:56 scheduler.py:648] Input prompt (2146 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:56 scheduler.py:648] Input prompt (2146 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:56 scheduler.py:648] Input prompt (2252 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:56 scheduler.py:648] Input prompt (2252 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  84%|████████▍ | 2830/3355 [07:26<00:59,  8.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:32:57 scheduler.py:648] Input prompt (2113 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:57 scheduler.py:648] Input prompt (2113 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:57 scheduler.py:648] Input prompt (2113 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:57 scheduler.py:648] Input prompt (2113 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  85%|████████▍ | 2836/3355 [07:26<00:48, 10.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:32:57 scheduler.py:648] Input prompt (2146 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:57 scheduler.py:648] Input prompt (2146 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  85%|████████▍ | 2840/3355 [07:27<01:01,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:32:58 scheduler.py:648] Input prompt (2289 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:58 scheduler.py:648] Input prompt (2289 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  85%|████████▍ | 2844/3355 [07:27<01:04,  7.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:32:59 scheduler.py:648] Input prompt (2113 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:59 scheduler.py:648] Input prompt (2113 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:59 scheduler.py:648] Input prompt (2113 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:59 scheduler.py:648] Input prompt (2113 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:59 scheduler.py:648] Input prompt (2289 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:59 scheduler.py:648] Input prompt (2289 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:59 scheduler.py:648] Input prompt (2130 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:32:59 scheduler.py:648] Input prompt (2130 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  85%|████████▌ | 2858/3355 [07:29<01:10,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:33:00 scheduler.py:648] Input prompt (2130 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:00 scheduler.py:648] Input prompt (2130 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:00 scheduler.py:648] Input prompt (2130 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:00 scheduler.py:648] Input prompt (2130 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  87%|████████▋ | 2908/3355 [07:39<01:56,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:33:10 scheduler.py:648] Input prompt (2302 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:10 scheduler.py:648] Input prompt (2302 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:10 scheduler.py:648] Input prompt (2302 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:10 scheduler.py:648] Input prompt (2302 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:10 scheduler.py:648] Input prompt (2843 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:10 scheduler.py:648] Input prompt (2843 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:10 scheduler.py:648] Input prompt (2843 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:10 scheduler.py:648] Input prompt (2843 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  87%|████████▋ | 2922/3355 [07:41<01:10,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:33:12 scheduler.py:648] Input prompt (2302 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:12 scheduler.py:648] Input prompt (2302 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:12 scheduler.py:648] Input prompt (2822 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:12 scheduler.py:648] Input prompt (2822 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:12 scheduler.py:648] Input prompt (2822 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:12 scheduler.py:648] Input prompt (2822 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:12 scheduler.py:648] Input prompt (2822 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:12 scheduler.py:648] Input prompt (2822 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:12 scheduler.py:648] Input prompt (3282 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:12 scheduler.py:648] Input prompt (3282 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:12 scheduler.py:648] Input prompt (3282 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:12 scheduler.py:648] Input prompt (3282 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:12 scheduler.py:648] Input prompt (2832 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:12 scheduler.py:648] Input prompt (2832 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  88%|████████▊ | 2938/3355 [07:41<00:22, 18.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:33:13 scheduler.py:648] Input prompt (2832 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:13 scheduler.py:648] Input prompt (2832 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:13 scheduler.py:648] Input prompt (2832 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:13 scheduler.py:648] Input prompt (2832 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  89%|████████▉ | 3000/3355 [07:54<01:33,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:33:25 scheduler.py:648] Input prompt (2419 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:25 scheduler.py:648] Input prompt (2419 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:25 scheduler.py:648] Input prompt (2223 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:25 scheduler.py:648] Input prompt (2223 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:25 scheduler.py:648] Input prompt (2281 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:25 scheduler.py:648] Input prompt (2281 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:25 scheduler.py:648] Input prompt (2783 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:25 scheduler.py:648] Input prompt (2783 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  90%|████████▉ | 3012/3355 [07:55<00:41,  8.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2922 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2922 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2785 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2785 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2785 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2785 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2419 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2419 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2979 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2979 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (3424 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (3424 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2842 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2842 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2223 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2223 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2783 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2783 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2783 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2783 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2726 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2726 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2726 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (2726 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (3424 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:26 scheduler.py:648] Input prompt (3424 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  91%|█████████ | 3044/3355 [07:56<00:21, 14.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2683 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2683 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2750 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2750 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2319 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2319 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  91%|█████████ | 3052/3355 [07:57<00:20, 14.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2683 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2683 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2750 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2750 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2688 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2688 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2688 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2688 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2688 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2688 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2990 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2990 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2626 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2626 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2626 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2626 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2626 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:28 scheduler.py:648] Input prompt (2626 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  92%|█████████▏| 3076/3355 [07:59<00:20, 13.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:33:30 scheduler.py:648] Input prompt (2343 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:30 scheduler.py:648] Input prompt (2343 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  93%|█████████▎| 3122/3355 [08:11<01:06,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:33:42 scheduler.py:648] Input prompt (2243 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:42 scheduler.py:648] Input prompt (2243 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  94%|█████████▍| 3157/3355 [08:20<00:55,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:33:51 scheduler.py:648] Input prompt (2199 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:51 scheduler.py:648] Input prompt (2199 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  94%|█████████▍| 3162/3355 [08:20<00:39,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:33:52 scheduler.py:648] Input prompt (2559 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:33:52 scheduler.py:648] Input prompt (2559 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  95%|█████████▌| 3200/3355 [08:30<00:44,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:34:02 scheduler.py:648] Input prompt (2194 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:02 scheduler.py:648] Input prompt (2194 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  96%|█████████▌| 3224/3355 [08:36<00:33,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:34:08 scheduler.py:648] Input prompt (2201 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:08 scheduler.py:648] Input prompt (2201 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:08 scheduler.py:648] Input prompt (2131 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:08 scheduler.py:648] Input prompt (2131 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:08 scheduler.py:648] Input prompt (2131 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:08 scheduler.py:648] Input prompt (2131 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:08 scheduler.py:648] Input prompt (2735 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:08 scheduler.py:648] Input prompt (2735 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  96%|█████████▋| 3236/3355 [08:38<00:14,  7.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:34:09 scheduler.py:648] Input prompt (2735 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:09 scheduler.py:648] Input prompt (2735 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:09 scheduler.py:648] Input prompt (2408 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:09 scheduler.py:648] Input prompt (2408 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:09 scheduler.py:648] Input prompt (2408 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:09 scheduler.py:648] Input prompt (2408 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  97%|█████████▋| 3260/3355 [08:42<00:26,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:34:14 scheduler.py:648] Input prompt (2443 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:14 scheduler.py:648] Input prompt (2443 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  98%|█████████▊| 3272/3355 [08:45<00:23,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:34:16 scheduler.py:648] Input prompt (2566 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:16 scheduler.py:648] Input prompt (2566 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:16 scheduler.py:648] Input prompt (2566 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:16 scheduler.py:648] Input prompt (2566 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:16 scheduler.py:648] Input prompt (2566 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:16 scheduler.py:648] Input prompt (2566 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:16 scheduler.py:648] Input prompt (2443 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:16 scheduler.py:648] Input prompt (2443 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:16 scheduler.py:648] Input prompt (2443 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:16 scheduler.py:648] Input prompt (2443 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:16 scheduler.py:648] Input prompt (2250 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:16 scheduler.py:648] Input prompt (2250 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:16 scheduler.py:648] Input prompt (2250 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:16 scheduler.py:648] Input prompt (2250 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:16 scheduler.py:648] Input prompt (2586 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:16 scheduler.py:648] Input prompt (2586 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:16 scheduler.py:648] Input prompt (2250 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:16 scheduler.py:648] Input prompt (2250 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:16 scheduler.py:648] Input prompt (2250 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:16 scheduler.py:648] Input prompt (2250 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  98%|█████████▊| 3294/3355 [08:46<00:02, 23.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:34:17 scheduler.py:648] Input prompt (2132 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:17 scheduler.py:648] Input prompt (2132 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:17 scheduler.py:648] Input prompt (2132 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:17 scheduler.py:648] Input prompt (2132 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:17 scheduler.py:648] Input prompt (2132 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:17 scheduler.py:648] Input prompt (2132 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  98%|█████████▊| 3302/3355 [08:46<00:02, 18.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 11:34:18 scheduler.py:648] Input prompt (2586 tokens) is too long and exceeds limit of 2048\n",
      "WARNING 05-29 11:34:18 scheduler.py:648] Input prompt (2586 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 3355/3355 [08:59<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "# mt-bench-type1\n",
    "import json\n",
    "texts_mtbench = []\n",
    "input_file_path = '/root/dataset/mt_bench_human.json'\n",
    "with open(input_file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        prompt = build_autoj_input(prompt=item['conversation_a'][0]['content'],\n",
    "                                   resp1=item['conversation_a'][1]['content'],\n",
    "                                   resp2=item['conversation_b'][1]['content'],\n",
    "                                   prompt2=item['conversation_a'][2]['content'],\n",
    "                                   resp1_2=item['conversation_a'][3]['content'],\n",
    "                                   resp2_2=item['conversation_b'][3]['content'],\n",
    "                                   protocol=\"multiturn_pairwise_tie_type1\")\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        texts_mtbench.append(text)\n",
    "outputs_mtbench = llm.generate(texts_mtbench, sampling_params)\n",
    "dataset_name = input_file_path.rsplit('/', 1)[-1].split('.')[0]\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/'\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.mkdir(output_file_path)\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/' + dataset_name + '_response_type1.json'\n",
    "with open(input_file_path, 'r') as fin:\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        for output, line in zip(outputs_mtbench, fin):\n",
    "            item = json.loads(line)\n",
    "            pred_label = extract_pariwise_result(output.outputs[0].text)\n",
    "            item['response_judge'] = output.outputs[0].text\n",
    "            item['pred_label'] = pred_label\n",
    "            json.dump(item, f)\n",
    "            f.write('\\n')\n",
    "            print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# mt-bench-type2\\nimport json\\ntexts_mtbench = []\\ninput_file_path = \\'/root/dataset/mt_bench_human.json\\'\\nwith open(input_file_path, \\'r\\') as f:\\n    for line in f:\\n        item = json.loads(line)\\n        prompt = build_autoj_input(prompt=item[\\'conversation_a\\'][0][\\'content\\'],\\n                                   resp1=item[\\'conversation_a\\'][1][\\'content\\'],\\n                                   resp2=item[\\'conversation_b\\'][1][\\'content\\'],\\n                                   prompt2=item[\\'conversation_a\\'][2][\\'content\\'],\\n                                   resp1_2=item[\\'conversation_a\\'][3][\\'content\\'],\\n                                   resp2_2=item[\\'conversation_b\\'][3][\\'content\\'],\\n                                   protocol=\"multiturn_pairwise_tie_type2\")\\n        messages = [\\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n        {\"role\": \"user\", \"content\": prompt}\\n        ]\\n        text = tokenizer.apply_chat_template(\\n            messages,\\n            tokenize=False,\\n            add_generation_prompt=True\\n        )\\n        texts_mtbench.append(text)\\noutputs_mtbench = llm.generate(texts_mtbench, sampling_params)\\ndataset_name = input_file_path.rsplit(\\'/\\', 1)[-1].split(\\'.\\')[0]\\noutput_file_path = input_file_path.rsplit(\\'/\\', 1)[0] + \\'/\\' + model_name + \\'/\\'\\nif not os.path.exists(output_file_path):\\n    os.mkdir(output_file_path)\\noutput_file_path = input_file_path.rsplit(\\'/\\', 1)[0] + \\'/\\' + model_name + \\'/\\' + dataset_name + \\'_response_type2.json\\'\\nwith open(input_file_path, \\'r\\') as fin:\\n    with open(output_file_path, \\'w\\') as f:\\n        for output, line in zip(outputs_mtbench, fin):\\n            item = json.loads(line)\\n            pred_label = extract_pariwise_result(output.outputs[0].text)\\n            item[\\'response_judge\\'] = output.outputs[0].text\\n            item[\\'pred_label\\'] = pred_label\\n            json.dump(item, f)\\n            f.write(\\'\\n\\')\\n            print(pred_label)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# mt-bench-type2\n",
    "import json\n",
    "texts_mtbench = []\n",
    "input_file_path = '/root/dataset/mt_bench_human.json'\n",
    "with open(input_file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        prompt = build_autoj_input(prompt=item['conversation_a'][0]['content'],\n",
    "                                   resp1=item['conversation_a'][1]['content'],\n",
    "                                   resp2=item['conversation_b'][1]['content'],\n",
    "                                   prompt2=item['conversation_a'][2]['content'],\n",
    "                                   resp1_2=item['conversation_a'][3]['content'],\n",
    "                                   resp2_2=item['conversation_b'][3]['content'],\n",
    "                                   protocol=\"multiturn_pairwise_tie_type2\")\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        texts_mtbench.append(text)\n",
    "outputs_mtbench = llm.generate(texts_mtbench, sampling_params)\n",
    "dataset_name = input_file_path.rsplit('/', 1)[-1].split('.')[0]\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/'\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.mkdir(output_file_path)\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/' + dataset_name + '_response_type2.json'\n",
    "with open(input_file_path, 'r') as fin:\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        for output, line in zip(outputs_mtbench, fin):\n",
    "            item = json.loads(line)\n",
    "            pred_label = extract_pariwise_result(output.outputs[0].text)\n",
    "            item['response_judge'] = output.outputs[0].text\n",
    "            item['pred_label'] = pred_label\n",
    "            json.dump(item, f)\n",
    "            f.write('\\n')\n",
    "            print(pred_label)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "judge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
