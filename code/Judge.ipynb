{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3,4,5'\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "from constants_prompt import build_autoj_input\n",
    "\n",
    "model_path_list = ['/root/Qwen1.5-7B-Chat', '/root/Meta-Llama-3-8B-Instruct', '/root/.cache/modelscope/hub/qwen/Qwen1___5-14B-Chat',\n",
    "                   '/root/Nanbeige2-8B-Chat', '/root/.cache/modelscope/hub/ZhipuAI/chatglm3-6b', \n",
    "                   '/root/.cache/modelscope/hub/Shanghai_AI_Laboratory/internlm2-chat-7b',\n",
    "                   '/root/.cache/modelscope/hub/deepseek-ai/deepseek-llm-7b-chat',\n",
    "                   '/root/.cache/modelscope/hub/lockonlvange/autoj-13b-fp16',\n",
    "                   '/root/.cache/huggingface/hub/models--WeOpenML--PandaLM-7B-v1/snapshots/PandaLM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = model_path_list[-1]\n",
    "llm = LLM(model=model_name_or_path, tensor_parallel_size=4, trust_remote_code=True, tokenizer_mode=\"auto\", dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=16384)\n",
    "model_name = model_name_or_path.rsplit('/', 1)[-1]\n",
    "from utils import extract_pariwise_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoj\n",
    "texts_autoj = []\n",
    "input_file_path = '/root/LLM-as-a-judge/datasets/AutoJ/testdata_pairwise.jsonl'\n",
    "with open(input_file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        prompt = build_autoj_input(prompt=item['prompt'], resp1=item['response 1'], resp2=item['response 2'], protocol=\"pairwise_tie\")\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        texts_autoj.append(text)\n",
    "outputs_autoj = llm.generate(texts_autoj, sampling_params)\n",
    "dataset_name = input_file_path.rsplit('/', 1)[-1].split('.')[0]\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/'\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.mkdir(output_file_path)\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/' + dataset_name + '_response.json'\n",
    "with open(input_file_path, 'r') as fin:\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        for output, line in zip(outputs_autoj, fin):\n",
    "            item = json.loads(line)\n",
    "            pred_label = extract_pariwise_result(output.outputs[0].text)\n",
    "            item['response_judge'] = output.outputs[0].text\n",
    "            item['pred_label'] = pred_label\n",
    "            json.dump(item, f)\n",
    "            f.write('\\n')\n",
    "            print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandalm\n",
    "prompt_templates = {\n",
    "    \"alpaca\": {\n",
    "        \"description\": \"Template used by Alpaca-LoRA.\",\n",
    "        \"prompt_input\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n",
    "        \"prompt_no_input\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n",
    "        \"response_split\": \"### Response:\",\n",
    "    }\n",
    "}\n",
    "def get_prompt(instruction, input):\n",
    "    prompt = prompt_templates['alpaca'][\"prompt_input\"].format(instruction=instruction, input=input)\n",
    "    return prompt\n",
    "\n",
    "\n",
    "texts_pandalm = []\n",
    "    \n",
    "import json\n",
    "input_file_path = '/root/PandaLM/data/testset-v1.json'\n",
    "with open(input_file_path, 'r') as f:\n",
    "    input_data_pandalm = json.load(f)\n",
    "    for item in input_data_pandalm:\n",
    "        prompt = build_autoj_input(prompt=get_prompt(item['instruction'], item['input']), resp1=item['response1'], resp2=item['response2'], protocol=\"pairwise_tie\")\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        texts_pandalm.append(text)\n",
    "\n",
    "outputs_pandalm = llm.generate(texts_pandalm, sampling_params)\n",
    "dataset_name = input_file_path.rsplit('/', 1)[-1].split('.')[0]\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/'\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.mkdir(output_file_path)\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/' + dataset_name + '_response.json'\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for output, item in zip(outputs_pandalm, input_data_pandalm):\n",
    "        pred_label = extract_pariwise_result(output.outputs[0].text)\n",
    "        item['response_judge'] = output.outputs[0].text\n",
    "        item['pred_label'] = pred_label\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')\n",
    "        print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llmbar_natural\n",
    "texts_natural = []\n",
    "input_file_path = '/root/LLMBar/Dataset/LLMBar/Natural/dataset.json'\n",
    "with open(input_file_path, 'r') as f:\n",
    "    input_data_llmbar_natural = json.load(f)\n",
    "    for item in input_data_llmbar_natural:\n",
    "        prompt = build_autoj_input(prompt=item['input'], resp1=item['output_1'], resp2=item['output_2'], protocol=\"pairwise_tie\")\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        texts_natural.append(text)\n",
    "outputs_llmbar_natural = llm.generate(texts_natural, sampling_params)\n",
    "dataset_name = input_file_path.rsplit('/', 1)[-1].split('.')[0]\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/'\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.mkdir(output_file_path)\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/' + dataset_name + '_response.json'\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for output, item in zip(outputs_llmbar_natural, input_data_llmbar_natural):\n",
    "        pred_label = extract_pariwise_result(output.outputs[0].text)\n",
    "        item['response_judge'] = output.outputs[0].text\n",
    "        item['pred_label'] = pred_label\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')\n",
    "        print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llmbar_neighbor\n",
    "\n",
    "texts_neighbor = []\n",
    "input_file_path = '/root/LLMBar/Dataset/LLMBar/Adversarial/Neighbor/dataset.json'\n",
    "with open(input_file_path, 'r') as f:\n",
    "    input_data_llmbar_neighbor = json.load(f)\n",
    "    for item in input_data_llmbar_neighbor:\n",
    "        prompt = build_autoj_input(prompt=item['input'], resp1=item['output_1'], resp2=item['output_2'], protocol=\"pairwise_tie\")\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        texts_neighbor.append(text)\n",
    "outputs_llmbar_neighbor = llm.generate(texts_neighbor, sampling_params)\n",
    "dataset_name = input_file_path.rsplit('/', 1)[-1].split('.')[0]\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/'\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.mkdir(output_file_path)\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/' + dataset_name + '_response.json'\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for output, item in zip(outputs_llmbar_neighbor, input_data_llmbar_neighbor):\n",
    "        pred_label = extract_pariwise_result(output.outputs[0].text)\n",
    "        item['response_judge'] = output.outputs[0].text\n",
    "        item['pred_label'] = pred_label\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')\n",
    "        print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llmbar_gptinst\n",
    "\n",
    "texts_gptinst = []\n",
    "input_file_path = '/root/LLMBar/Dataset/LLMBar/Adversarial/GPTInst/dataset.json'\n",
    "with open(input_file_path, 'r') as f:\n",
    "    input_data_llmbar_gptinst = json.load(f)\n",
    "    for item in input_data_llmbar_gptinst:\n",
    "        prompt = build_autoj_input(prompt=item['input'], resp1=item['output_1'], resp2=item['output_2'], protocol=\"pairwise_tie\")\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        texts_gptinst.append(text)\n",
    "outputs_llmbar_gptinst = llm.generate(texts_gptinst, sampling_params)\n",
    "dataset_name = input_file_path.rsplit('/', 1)[-1].split('.')[0]\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/'\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.mkdir(output_file_path)\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/' + dataset_name + '_response.json'\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for output, item in zip(outputs_llmbar_gptinst, input_data_llmbar_gptinst):\n",
    "        pred_label = extract_pariwise_result(output.outputs[0].text)\n",
    "        item['response_judge'] = output.outputs[0].text\n",
    "        item['pred_label'] = pred_label\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')\n",
    "        print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llmbar_gptout\n",
    "\n",
    "texts_gptout = []\n",
    "input_file_path = '/root/LLMBar/Dataset/LLMBar/Adversarial/GPTOut/dataset.json'\n",
    "with open(input_file_path, 'r') as f:\n",
    "    input_data_llmbar_gptout = json.load(f)\n",
    "    for item in input_data_llmbar_gptout:\n",
    "        prompt = build_autoj_input(prompt=item['input'], resp1=item['output_1'], resp2=item['output_2'], protocol=\"pairwise_tie\")\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        texts_gptout.append(text)\n",
    "outputs_llmbar_gptout = llm.generate(texts_gptout, sampling_params)\n",
    "dataset_name = input_file_path.rsplit('/', 1)[-1].split('.')[0]\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/'\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.mkdir(output_file_path)\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/' + dataset_name + '_response.json'\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for output, item in zip(outputs_llmbar_gptout, input_data_llmbar_gptout):\n",
    "        pred_label = extract_pariwise_result(output.outputs[0].text)\n",
    "        item['response_judge'] = output.outputs[0].text\n",
    "        item['pred_label'] = pred_label\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')\n",
    "        print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llmbar_manual\n",
    "\n",
    "texts_manual = []\n",
    "input_file_path = '/root/LLMBar/Dataset/LLMBar/Adversarial/Manual/dataset.json'\n",
    "with open(input_file_path, 'r') as f:\n",
    "    input_data_llmbar_manual = json.load(f)\n",
    "    for item in input_data_llmbar_manual:\n",
    "        prompt = build_autoj_input(prompt=item['input'], resp1=item['output_1'], resp2=item['output_2'], protocol=\"pairwise_tie\")\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        texts_manual.append(text)\n",
    "outputs_llmbar_manual = llm.generate(texts_manual, sampling_params)\n",
    "dataset_name = input_file_path.rsplit('/', 1)[-1].split('.')[0]\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/'\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.mkdir(output_file_path)\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/' + dataset_name + '_response.json'\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for output, item in zip(outputs_llmbar_manual, input_data_llmbar_manual):\n",
    "        pred_label = extract_pariwise_result(output.outputs[0].text)\n",
    "        item['response_judge'] = output.outputs[0].text\n",
    "        item['pred_label'] = pred_label\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')\n",
    "        print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mt-bench-type1\n",
    "import json\n",
    "texts_mtbench = []\n",
    "input_file_path = '/root/dataset/mt_bench_human.json'\n",
    "with open(input_file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        prompt = build_autoj_input(prompt=item['conversation_a'][0]['content'],\n",
    "                                   resp1=item['conversation_a'][1]['content'],\n",
    "                                   resp2=item['conversation_b'][1]['content'],\n",
    "                                   prompt2=item['conversation_a'][2]['content'],\n",
    "                                   resp1_2=item['conversation_a'][3]['content'],\n",
    "                                   resp2_2=item['conversation_b'][3]['content'],\n",
    "                                   protocol=\"multiturn_pairwise_tie_type1\")\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        texts_mtbench.append(text)\n",
    "outputs_mtbench = llm.generate(texts_mtbench, sampling_params)\n",
    "dataset_name = input_file_path.rsplit('/', 1)[-1].split('.')[0]\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/'\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.mkdir(output_file_path)\n",
    "output_file_path = input_file_path.rsplit('/', 1)[0] + '/' + model_name + '/' + dataset_name + '_response_type1.json'\n",
    "with open(input_file_path, 'r') as fin:\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        for output, line in zip(outputs_mtbench, fin):\n",
    "            item = json.loads(line)\n",
    "            pred_label = extract_pariwise_result(output.outputs[0].text)\n",
    "            item['response_judge'] = output.outputs[0].text\n",
    "            item['pred_label'] = pred_label\n",
    "            json.dump(item, f)\n",
    "            f.write('\\n')\n",
    "            print(pred_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "judge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
