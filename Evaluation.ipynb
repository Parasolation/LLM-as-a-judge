{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "model_path_list = ['/root/Qwen1.5-7B-Chat', '/root/Meta-Llama-3-8B-Instruct', '/root/.cache/modelscope/hub/qwen/Qwen1___5-14B-Chat',\n",
    "                   '/root/Nanbeige2-8B-Chat', '/root/.cache/modelscope/hub/ZhipuAI/chatglm3-6b', \n",
    "                   '/root/.cache/modelscope/hub/Shanghai_AI_Laboratory/internlm2-chat-7b',\n",
    "                   '/root/.cache/modelscope/hub/deepseek-ai/deepseek-llm-7b-chat',\n",
    "                   '/root/.cache/modelscope/hub/lockonlvange/autoj-13b-fp16',\n",
    "                   '/root/PandaLM-7B-v1']\n",
    "#finetuned_model_path_list = ['/root/finetuned/qwen/Qwen1.5-14B-Chat']\n",
    "model_name_or_path = model_path_list[0]\n",
    "model_name = model_name_or_path.rsplit('/', 1)[-1]\n",
    "folder = '/root/evaluation'\n",
    "evaluation_out_folder = f'{folder}/{model_name}'.replace('//', '/')\n",
    "evaluation_out_path = f'{folder}/{model_name}/evaluation.json'.replace('//', '/')\n",
    "if not os.path.exists(evaluation_out_folder):\n",
    "    os.mkdir(evaluation_out_folder)\n",
    "fout = open(evaluation_out_path, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoj acc: 42.53\n",
      "format error: 114, total: 1392\n"
     ]
    }
   ],
   "source": [
    "# autoj\n",
    "response_file_path = f'/root/auto-j/data/test/{model_name}/testdata_pairwise_response.json'\n",
    "with open(response_file_path, 'r') as f:\n",
    "    allin = f.readlines()\n",
    "    correct = 0\n",
    "    format_error = 0\n",
    "    for line in allin:\n",
    "        line_json = json.loads(line)\n",
    "        label = line_json['label']\n",
    "        pred = line_json['pred_label']\n",
    "        correct += 1 if label == pred else 0\n",
    "        format_error += 1 if pred == -1 else 0\n",
    "    print(f'autoj acc: {100 * correct / len(allin):.2f}')\n",
    "    print(f'format error: {format_error}, total: {len(allin)}')\n",
    "    fout.write(f'autoj acc: {100 * correct / len(allin):.2f}\\nformat error: {format_error}, total: {len(allin)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandalm acc: 51.65\n",
      "format error: 50, total: 999\n"
     ]
    }
   ],
   "source": [
    "# pandalm\n",
    "response_file_path = f'/root/PandaLM/data/{model_name}/testset-v1_response.json'\n",
    "with open(response_file_path, 'r') as f:\n",
    "    allin = f.readlines()\n",
    "    correct = 0\n",
    "    format_error = 0\n",
    "    for line in allin:\n",
    "        line_json = json.loads(line)\n",
    "        lable_list = [0, 0, 0]\n",
    "        for i in range(3):\n",
    "            lable_list[line_json[f'annotator{i+1}']] += 1\n",
    "        if(max(lable_list) == 1):\n",
    "            correct += 1\n",
    "        else:\n",
    "            label = (lable_list.index(max(lable_list)) + 2) % 3\n",
    "        pred = line_json['pred_label']\n",
    "        correct += 1 if label == pred else 0\n",
    "        format_error += 1 if pred == -1 else 0\n",
    "    print(f'pandalm acc: {100 * correct / len(allin):.2f}')\n",
    "    print(f'format error: {format_error}, total: {len(allin)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llmbar_natural acc: 52.00\n",
      "format error: 13, total: 100\n"
     ]
    }
   ],
   "source": [
    "# llmbar_natural\n",
    "response_file_path = f'/root/LLMBar/Dataset/LLMBar/Natural/{model_name}/dataset_response.json'\n",
    "with open(response_file_path, 'r') as f:\n",
    "    allin = f.readlines()\n",
    "    correct = 0\n",
    "    format_error = 0\n",
    "    for line in allin:\n",
    "        line_json = json.loads(line)\n",
    "        label = line_json['label'] - 1\n",
    "        pred = line_json['pred_label']\n",
    "        correct += 1 if label == pred else 0\n",
    "        format_error += 1 if pred == -1 else 0\n",
    "    print(f'llmbar_natural acc: {100 * correct / len(allin):.2f}')\n",
    "    print(f'format error: {format_error}, total: {len(allin)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llmbar_neighbor acc: 23.88\n",
      "format error: 13, total: 134\n"
     ]
    }
   ],
   "source": [
    "# llmbar_neighbor\n",
    "response_file_path = f'/root/LLMBar/Dataset/LLMBar/Adversarial/Neighbor/{model_name}/dataset_response.json'\n",
    "with open(response_file_path, 'r') as f:\n",
    "    allin = f.readlines()\n",
    "    correct = 0\n",
    "    format_error = 0\n",
    "    for line in allin:\n",
    "        line_json = json.loads(line)\n",
    "        label = line_json['label'] - 1\n",
    "        pred = line_json['pred_label']\n",
    "        correct += 1 if label == pred else 0\n",
    "        format_error += 1 if pred == -1 else 0\n",
    "    print(f'llmbar_neighbor acc: {100 * correct / len(allin):.2f}')\n",
    "    print(f'format error: {format_error}, total: {len(allin)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llmbar_gptinst acc: 33.70\n",
      "format error: 11, total: 92\n"
     ]
    }
   ],
   "source": [
    "# llmbar_gptinst\n",
    "response_file_path = f'/root/LLMBar/Dataset/LLMBar/Adversarial/GPTInst/{model_name}/dataset_response.json'\n",
    "with open(response_file_path, 'r') as f:\n",
    "    allin = f.readlines()\n",
    "    correct = 0\n",
    "    format_error = 0\n",
    "    for line in allin:\n",
    "        line_json = json.loads(line)\n",
    "        label = line_json['label'] - 1\n",
    "        pred = line_json['pred_label']\n",
    "        correct += 1 if label == pred else 0\n",
    "        format_error += 1 if pred == -1 else 0\n",
    "    print(f'llmbar_gptinst acc: {100 * correct / len(allin):.2f}')\n",
    "    print(f'format error: {format_error}, total: {len(allin)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llmbar_gptout acc: 48.94\n",
      "format error: 4, total: 47\n"
     ]
    }
   ],
   "source": [
    "# llmbar_gptout\n",
    "response_file_path = f'/root/LLMBar/Dataset/LLMBar/Adversarial/GPTOut/{model_name}/dataset_response.json'\n",
    "with open(response_file_path, 'r') as f:\n",
    "    allin = f.readlines()\n",
    "    correct = 0\n",
    "    format_error = 0\n",
    "    for line in allin:\n",
    "        line_json = json.loads(line)\n",
    "        label = line_json['label'] - 1\n",
    "        pred = line_json['pred_label']\n",
    "        correct += 1 if label == pred else 0\n",
    "        format_error += 1 if pred == -1 else 0\n",
    "    print(f'llmbar_gptout acc: {100 * correct / len(allin):.2f}')\n",
    "    print(f'format error: {format_error}, total: {len(allin)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llmbar_manual acc: 32.61\n",
      "format error: 2, total: 46\n"
     ]
    }
   ],
   "source": [
    "# llmbar_manual\n",
    "response_file_path = f'/root/LLMBar/Dataset/LLMBar/Adversarial/Manual/{model_name}/dataset_response.json'\n",
    "with open(response_file_path, 'r') as f:\n",
    "    allin = f.readlines()\n",
    "    correct = 0\n",
    "    format_error = 0\n",
    "    for line in allin:\n",
    "        line_json = json.loads(line)\n",
    "        label = line_json['label'] - 1\n",
    "        pred = line_json['pred_label']\n",
    "        correct += 1 if label == pred else 0\n",
    "        format_error += 1 if pred == -1 else 0\n",
    "    print(f'llmbar_manual acc: {100 * correct / len(allin):.2f}')\n",
    "    print(f'format error: {format_error}, total: {len(allin)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mt-bench-type1 acc: 36.15\n",
      "format error: 1109, total: 3355\n"
     ]
    }
   ],
   "source": [
    "# mt-bench-type1\n",
    "response_file_path = f'/root/dataset/{model_name}/mt_bench_human_response_type1.json'\n",
    "with open(response_file_path, 'r') as f:\n",
    "    allin = f.readlines()\n",
    "    correct = 0\n",
    "    format_error = 0\n",
    "    for line in allin:\n",
    "        line_json = json.loads(line)\n",
    "        winner = line_json['winner']\n",
    "        if winner == 'model_a':\n",
    "            label = 0\n",
    "        elif winner == 'model_b':\n",
    "            label = 1\n",
    "        elif winner == 'tie':\n",
    "            label = 2\n",
    "        else:\n",
    "            print('error')\n",
    "        pred = line_json['pred_label']\n",
    "        correct += 1 if label == pred else 0\n",
    "        format_error += 1 if pred == -1 else 0\n",
    "    print(f'mt-bench-type1 acc: {100 * correct / len(allin):.2f}')\n",
    "    print(f'format error: {format_error}, total: {len(allin)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# mt-bench-type2\\nresponse_file_path = f'/root/dataset/{model_name}/mt_bench_human_response_type2.json'\\nwith open(response_file_path, 'r') as f:\\n    allin = f.readlines()\\n    correct = 0\\n    format_error = 0\\n    for line in allin:\\n        line_json = json.loads(line)\\n        winner = line_json['winner']\\n        if winner == 'model_a':\\n            label = 0\\n        elif winner == 'model_b':\\n            label = 1\\n        elif winner == 'tie':\\n            label = 2\\n        else:\\n            print('error')\\n        pred = line_json['pred_label']\\n        correct += 1 if label == pred else 0\\n        format_error += 1 if pred == -1 else 0\\n    print(f'mt-bench-type2 acc: {100 * correct / len(allin):.2f}')\\n    print(f'format error: {format_error}, total: {len(allin)}')\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# mt-bench-type2\n",
    "response_file_path = f'/root/dataset/{model_name}/mt_bench_human_response_type2.json'\n",
    "with open(response_file_path, 'r') as f:\n",
    "    allin = f.readlines()\n",
    "    correct = 0\n",
    "    format_error = 0\n",
    "    for line in allin:\n",
    "        line_json = json.loads(line)\n",
    "        winner = line_json['winner']\n",
    "        if winner == 'model_a':\n",
    "            label = 0\n",
    "        elif winner == 'model_b':\n",
    "            label = 1\n",
    "        elif winner == 'tie':\n",
    "            label = 2\n",
    "        else:\n",
    "            print('error')\n",
    "        pred = line_json['pred_label']\n",
    "        correct += 1 if label == pred else 0\n",
    "        format_error += 1 if pred == -1 else 0\n",
    "    print(f'mt-bench-type2 acc: {100 * correct / len(allin):.2f}')\n",
    "    print(f'format error: {format_error}, total: {len(allin)}')\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "judge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
